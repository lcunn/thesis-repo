{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PianoRollConvEncoder(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 2, kernel_size=(10, 10), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(2, 4, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (fc): Linear(in_features=20768, out_features=64, bias=True)\n",
      ")\n",
      "ProjectionHead(\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "SiameseModel(\n",
      "  (encoder): PianoRollConvEncoder(\n",
      "    (conv_layers): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(10, 10), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(2, 4, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU()\n",
      "    )\n",
      "    (fc): Linear(in_features=20768, out_features=64, bias=True)\n",
      "  )\n",
      "  (projection_head): ProjectionHead(\n",
      "    (projector): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.chdir(\"c:/Users/cunn2/OneDrive/DSML/Project/thesis-repo\")\n",
    "\n",
    "from sms.exp1.config_classes import load_config_from_launchplan\n",
    "from sms.exp1.run_training import build_encoder, build_projector\n",
    "from sms.exp1.models.siamese import SiameseModel\n",
    "\n",
    "config = load_config_from_launchplan(\"sms/exp1/runs/run_20240926_162652/original_launchplan.yaml\")\n",
    "\n",
    "encoder = build_encoder(config.model_dump())\n",
    "projector = build_projector(config.model_dump())\n",
    "\n",
    "model = SiameseModel(encoder, projector)\n",
    "\n",
    "print(encoder)\n",
    "print(projector)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_133096\\885777492.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pt_encoder.load_state_dict(torch.load(\"sms/exp1/runs/run_20240926_162652/pretrain_saved_model.pth\"))\n",
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_133096\\885777492.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ft_encoder.load_state_dict(torch.load(\"sms/exp1/runs/run_20240926_162652/finetune_saved_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_encoder = build_encoder(config.model_dump())\n",
    "pt_encoder.load_state_dict(torch.load(\"sms/exp1/runs/run_20240926_162652/pretrain_saved_model.pth\"))    \n",
    "\n",
    "ft_encoder = build_encoder(config.model_dump())\n",
    "ft_encoder.load_state_dict(torch.load(\"sms/exp1/runs/run_20240926_162652/finetune_saved_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_133096\\1356997116.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\data\\exp1\\train_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\data\\exp1\\train_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2, 67. ],\n",
       "       [ 1. , 74. ],\n",
       "       [ 2. , 76. ],\n",
       "       [ 0.8, 74. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-28 20:47:51] [DEBUG] Transposing non-rest notes by 10 semitones.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2 67. ]\n",
      " [ 1.  74. ]\n",
      " [ 2.  76. ]\n",
      " [ 0.8 74. ]]\n",
      "tensor([[ 0.2000, 77.0000],\n",
      "        [ 1.0000, 84.0000],\n",
      "        [ 2.0000, 86.0000],\n",
      "        [ 0.8000, 84.0000]], dtype=torch.float64)\n",
      "[[ 0.75 67.  ]\n",
      " [ 0.25 69.  ]\n",
      " [ 1.   71.  ]\n",
      " [ 0.5  67.  ]\n",
      " [ 0.5  67.  ]\n",
      " [ 1.   69.  ]]\n",
      "pos distance: 29.393085479736328\n",
      "neg distance: 20.81243324279785\n"
     ]
    }
   ],
   "source": [
    "from sms.src.synthetic_data.formatter import InputFormatter\n",
    "from sms.src.synthetic_data.note_arr_mod import NoteArrayModifier\n",
    "import numpy as np\n",
    "import logging\n",
    "from sms.src.log import configure_logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "configure_logging(console_level=logging.DEBUG)\n",
    "\n",
    "formatter = InputFormatter(**config.model_dump()['input'])\n",
    "\n",
    "aug_dict = {\n",
    "    \"use_transposition\": True,\n",
    "    \"use_shift_selected_notes_pitch\": False,\n",
    "    \"use_change_note_durations\": False,\n",
    "    \"use_delete_notes\": False,\n",
    "    \"use_insert_notes\": False\n",
    "}\n",
    "\n",
    "modifier = NoteArrayModifier()\n",
    "\n",
    "def format_data(data: np.ndarray):\n",
    "    return formatter(data).astype(np.float32).copy()\n",
    "\n",
    "anchor = data[0]\n",
    "pos = modifier(anchor, aug_dict)\n",
    "neg = data[18]\n",
    "print(anchor)\n",
    "print(pos)\n",
    "print(neg)\n",
    "\n",
    "anchor = format_data(anchor)\n",
    "pos = format_data(pos)\n",
    "neg = format_data(data[17])\n",
    "\n",
    "anchor_enc = ft_encoder((torch.from_numpy(anchor)).unsqueeze(0))[0].detach().numpy()   \n",
    "pos_enc = ft_encoder((torch.from_numpy(pos)).unsqueeze(0))[0].detach().numpy()\n",
    "neg_enc = ft_encoder((torch.from_numpy(neg)).unsqueeze(0))[0].detach().numpy()\n",
    "\n",
    "print(f'pos distance: {np.linalg.norm(anchor_enc - pos_enc)}')\n",
    "print(f'neg distance: {np.linalg.norm(anchor_enc - neg_enc)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def format_data_for_conv_enc(data: np.ndarray, formatter: InputFormatter):\n",
    "    return torch.from_numpy(formatter(data).astype(np.float32).copy())\n",
    "\n",
    "def format_dataset_for_conv_enc(dataset: List[np.ndarray]):\n",
    "    formatted_data = [format_data_for_conv_enc(data, formatter) for data in dataset]\n",
    "    return torch.stack(formatted_data, dim=0)\n",
    "\n",
    "data_formatted = format_dataset_for_conv_enc(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14631, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "print(data_formatted.shape)\n",
    "embeddings = ft_encoder(data_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14631, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.8189e+00, -1.0568e+01,  1.3061e+00, -3.7155e+00,  2.3748e+00,\n",
       "        -9.1931e+00,  4.0135e+00,  7.2098e+00,  1.6388e+00,  1.6952e+01,\n",
       "         3.5173e+00, -3.6373e+00,  3.9732e+00,  1.5120e+01,  1.0261e+00,\n",
       "         1.1293e+01,  6.1510e+00, -4.7417e+00, -4.8012e+00, -1.4328e+01,\n",
       "        -2.8075e+00, -3.0592e+00, -7.0917e+00, -7.3687e+00,  1.3666e+00,\n",
       "        -2.9715e+00,  4.9831e+00,  5.1463e+00, -3.3394e+00, -6.1344e+00,\n",
       "        -6.1643e+00, -1.6928e+01, -2.0705e+00,  7.5320e-01, -7.7485e+00,\n",
       "        -9.8449e+00,  4.6780e+00,  3.3799e+00,  4.8790e+00, -1.0553e-02,\n",
       "         2.3219e+00,  1.0051e+01, -8.1662e+00,  1.1222e+01,  1.2169e+00,\n",
       "         2.3066e+00, -1.3469e+01,  4.2552e-01, -3.1998e+00, -1.3172e+00,\n",
       "        -1.8167e+00,  6.6501e+00,  8.4608e-01,  1.6913e+00, -3.8155e+00,\n",
       "        -4.9456e+00, -9.2409e+00,  9.4063e+00,  9.8553e+00,  3.5024e+00,\n",
       "        -1.2613e+01, -2.1675e+01, -5.5904e+00, -1.0875e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-28 20:49:08] [DEBUG] Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "[2024-09-28 20:49:08] [INFO ] Loading faiss with AVX512 support.\n",
      "[2024-09-28 20:49:08] [INFO ] Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "[2024-09-28 20:49:08] [INFO ] Loading faiss with AVX2 support.\n",
      "[2024-09-28 20:49:08] [INFO ] Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "[2024-09-28 20:49:08] [INFO ] Loading faiss.\n",
      "[2024-09-28 20:49:08] [INFO ] Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CustomFAISSIndex:\n",
    "    def __init__(self, index_type: str, index_args: List[Any] = [], index_kwargs: Dict[str, Any] = {}):\n",
    "        self.index = getattr(faiss, index_type)(*index_args, **index_kwargs)\n",
    "        self.id_to_index = {}  # Maps custom IDs to FAISS indices\n",
    "        self.index_to_id = {}  # Maps FAISS indices to custom IDs\n",
    "        self.id_to_data = {}   # Maps custom IDs to original data\n",
    "\n",
    "    def add_with_id(self, id, vector, original_data=None):\n",
    "        if id in self.id_to_index:\n",
    "            raise ValueError(f\"ID {id} already exists in the index\")\n",
    "        \n",
    "        index = self.index.ntotal\n",
    "        self.index.add(np.array([vector], dtype=np.float32))\n",
    "        self.id_to_index[id] = index\n",
    "        self.index_to_id[index] = id\n",
    "        if original_data is not None:\n",
    "            self.id_to_data[id] = original_data\n",
    "\n",
    "    def remove(self, id):\n",
    "        if id not in self.id_to_index:\n",
    "            raise ValueError(f\"ID {id} not found in the index\")\n",
    "        \n",
    "        index_to_remove = self.id_to_index[id]\n",
    "        self.index.remove_ids(np.array([index_to_remove]))\n",
    "        \n",
    "        # Update mappings\n",
    "        del self.index_to_id[index_to_remove]\n",
    "        del self.id_to_index[id]\n",
    "        if id in self.id_to_data:\n",
    "            del self.id_to_data[id]\n",
    "        \n",
    "        # Update remaining indices\n",
    "        for i in range(index_to_remove, self.index.ntotal):\n",
    "            old_id = self.index_to_id[i + 1]\n",
    "            self.index_to_id[i] = old_id\n",
    "            self.id_to_index[old_id] = i\n",
    "        del self.index_to_id[self.index.ntotal]\n",
    "\n",
    "    def search(self, query_vector, k,):\n",
    "        distances, indices = self.index.search(np.array([query_vector], dtype=np.float32), k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx != -1 and idx in self.index_to_id:\n",
    "                id = self.index_to_id[idx]\n",
    "                results.append((id, self.id_to_data.get(id)))\n",
    "        return results\n",
    "\n",
    "    def get_vector(self, id):\n",
    "        if id not in self.id_to_index:\n",
    "            raise ValueError(f\"ID {id} not found in the index\")\n",
    "        index = self.id_to_index[id]\n",
    "        return self.index.reconstruct(index)\n",
    "\n",
    "    def get_original_data(self, id):\n",
    "        return self.id_to_data.get(id)\n",
    "    \n",
    "    def get_all_items(self, limit=3):\n",
    "        items = []\n",
    "        for id in list(self.id_to_data.keys())[:limit]:  # Limit the number of items\n",
    "            vector = self.get_vector(id)\n",
    "            original_data = self.get_original_data(id)\n",
    "            items.append((id, vector, original_data))\n",
    "        return items\n",
    "\n",
    "    def __repr__(self):\n",
    "        items = self.get_all_items(limit=3)  # Limit to 3 items\n",
    "        total_items = self.index.ntotal\n",
    "        repr_str = f\"CustomFAISSIndex with {total_items} items:\\n\"\n",
    "        for id, vector, original_data in items:\n",
    "            repr_str += f\"  ID: {id}\\n\"\n",
    "            repr_str += f\"    Vector: {vector}\\n\"\n",
    "            repr_str += f\"    Original Data: {original_data}\\n\"\n",
    "        if total_items > 3:\n",
    "            repr_str += f\"  ... and {total_items - 3} more items\\n\"\n",
    "        return repr_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "data_ids = [str(uuid4()) for _ in range(len(data))]\n",
    "data_dict = dict(zip(data_ids, data))\n",
    "embeddings_dict = dict(zip(data_ids, embeddings.detach().numpy()))\n",
    "\n",
    "dim = list(embeddings_dict.values())[0].shape[0]\n",
    "embedding_index = CustomFAISSIndex(index_type=\"IndexLSH\", index_args=[dim, 256])\n",
    "for key, value in embeddings_dict.items():\n",
    "    embedding_index.add_with_id(key, value, data_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2, 67. ],\n",
       "       [ 1. , 74. ],\n",
       "       [ 2. , 76. ],\n",
       "       [ 0.8, 74. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index.get_original_data(data_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1: Verify all documents are added\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'custom_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck 1: Verify all documents are added\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc3\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     vector \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_index\u001b[49m\u001b[38;5;241m.\u001b[39mget_vector(doc_id)\n\u001b[0;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m custom_index\u001b[38;5;241m.\u001b[39mget_original_data(doc_id)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Vector = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Data = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_index' is not defined"
     ]
    }
   ],
   "source": [
    "# Check 1: Verify all documents are added\n",
    "print(\"Check 1: Verify all documents are added\")\n",
    "for doc_id in [\"doc1\", \"doc2\", \"doc3\"]:\n",
    "    vector = embedding_index.get_vector(doc_id)\n",
    "    data = embedding_index.get_original_data(doc_id)\n",
    "    print(f\"{doc_id}: Vector = {vector}, Data = {data}\")\n",
    "\n",
    "# Check 2: Remove a document and verify it's gone\n",
    "print(\"\\nCheck 2: Remove a document and verify it's gone\")\n",
    "embedding_index.remove(\"doc2\")\n",
    "try:\n",
    "    embedding_index.get_vector(\"doc2\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")\n",
    "\n",
    "# Check 3: Verify remaining documents are still accessible\n",
    "print(\"\\nCheck 3: Verify remaining documents are still accessible\")\n",
    "for doc_id in [\"doc1\", \"doc3\"]:\n",
    "    vector = embedding_index.get_vector(doc_id)\n",
    "    data = embedding_index.get_original_data(doc_id)\n",
    "    print(f\"{doc_id}: Vector = {vector}, Data = {data}\")\n",
    "\n",
    "# Check 4: Add a new document and verify it's added correctly\n",
    "print(\"\\nCheck 4: Add a new document and verify it's added correctly\")\n",
    "embedding_index.add_with_id(\"doc4\", np.array([4] * dim), 4)\n",
    "embedding_index.add_with_id(\"doc5\", np.array([5] * dim), 5)\n",
    "vector = embedding_index.get_vector(\"doc4\")\n",
    "data = embedding_index.get_original_data(\"doc4\")\n",
    "print(f\"doc4: Vector = {vector}, Data = {data}\")\n",
    "\n",
    "# Check 5: Perform a search and verify results\n",
    "print(\"\\nCheck 5: Perform a search and verify results\")\n",
    "query_vector = np.array([2.5] * dim)\n",
    "results = embedding_index.search(query_vector, k=2)\n",
    "print(f\"Search results for query {query_vector}:\")\n",
    "for id, data in results:\n",
    "    print(f\"ID: {id}, Data: {data}\")\n",
    "\n",
    "# Check 6: Try to add a document with an existing ID (should raise an error)\n",
    "print(\"\\nCheck 6: Try to add a document with an existing ID\")\n",
    "try:\n",
    "    embedding_index.add_with_id(\"doc1\", np.array([5] * dim), 5)\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")\n",
    "\n",
    "# Check 7: Try to remove a non-existent document (should raise an error)\n",
    "print(\"\\nCheck 7: Try to remove a non-existent document\")\n",
    "try:\n",
    "    embedding_index.remove(\"doc5\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp1 eval loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'uuid4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# produce vector embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'uuid4'"
     ]
    }
   ],
   "source": [
    "# produce vector embeddings\n",
    "from uuid import uuid4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Callable, Optional, List, Dict\n",
    "from sms.src.synthetic_data.formatter import InputFormatter\n",
    "from sms.src.synthetic_data.note_arr_mod import NoteArrayModifier\n",
    "\n",
    "from sms.exp1.run_training import build_encoder, build_projector\n",
    "from sms.exp1.models.siamese import SiameseModel\n",
    "\n",
    "data_ids = [str(uuid4()) for _ in range(len(data))]\n",
    "data_dict = dict(zip(data_ids, data))\n",
    "embeddings_dict = dict(zip(data_ids, embeddings.detach().numpy()))\n",
    "\n",
    "num_loops = 1000\n",
    "anchor_keys = np.random.choice(list(embeddings_dict.keys()), size=num_loops, replace=False)\n",
    "\n",
    "def augment_chunk(chunk: np.ndarray, augmentation: str):\n",
    "    \"\"\" \n",
    "    augmentation is one of the following:\n",
    "        use_transposition\n",
    "        use_shift_selected_notes_pitch\n",
    "        use_change_note_durations\n",
    "        use_delete_notes\n",
    "        use_insert_notes\n",
    "    \"\"\"\n",
    "    aug_dict = {\n",
    "        \"use_transposition\": False,\n",
    "        \"use_shift_selected_notes_pitch\": False,\n",
    "        \"use_change_note_durations\": False,\n",
    "        \"use_delete_notes\": False,\n",
    "        \"use_insert_notes\": False\n",
    "    }\n",
    "    aug_dict[augmentation] = True\n",
    "    modifier = NoteArrayModifier()\n",
    "    return modifier(chunk, aug_dict)\n",
    "\n",
    "def create_augmented_data(data_dict: Dict[str, np.ndarray], anchor_keys: List[str]) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Create the augmented data for the given anchor keys.\n",
    "    Returns a dictionary of dictionaries, where the outer dictionary is keyed by the anchor keys, and the inner dictionary \n",
    "        is keyed by the type of augmentation and contains the augmented data.\n",
    "    \"\"\"\n",
    "    augmented_data = {}\n",
    "    for key in anchor_keys:\n",
    "        chunk = data_dict[key]\n",
    "        augmented_data[key] = {\n",
    "            \"chunk_transposed\": augment_chunk(chunk, \"use_transposition\"),\n",
    "            \"chunk_one_pitch_shifted\": augment_chunk(chunk, \"use_shift_selected_notes_pitch\"),\n",
    "            \"chunk_note_duration_changed\": augment_chunk(chunk, \"use_change_note_durations\"),\n",
    "            \"chunk_note_deleted\": augment_chunk(chunk, \"use_delete_notes\"),\n",
    "            \"chunk_note_inserted\": augment_chunk(chunk, \"use_insert_notes\")\n",
    "        }\n",
    "    return augmented_data\n",
    "\n",
    "def build_model(dumped_lp_config: Dict[str, Any], model_path: str, use_pt: bool = False):\n",
    "    \"\"\"\n",
    "    If use_pt is true, we assume the model_path contains weights for the full Siamese model.\n",
    "    If use_pt is false, we assume the model_path contains weights for the encoder only.\n",
    "    \"\"\"\n",
    "    encoder = build_encoder(dumped_lp_config)\n",
    "    projector = build_projector(dumped_lp_config)\n",
    "    model = SiameseModel(encoder, projector)\n",
    "    if use_pt:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        model = model.get_encoder()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "def create_embedding_dicts(data_dict: Dict[str, np.ndarray], dumped_lp_config: Dict[str, Any], model: Callable) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create the embedding dictionary for the given model. The dumped_lp_config is used to determine the input format of the model.\n",
    "    \"\"\"\n",
    "    formatter = InputFormatter(**dumped_lp_config['input'])\n",
    "    formatted_data_list = [torch.from_numpy(formatter(chunk).astype(np.float32).copy()) for chunk in data_dict.values()]\n",
    "    formatted_data_stacked = torch.stack(formatted_data_list, dim=0) # shape [num_chunks, *input_shape]\n",
    "    embeddings_stacked = model(formatted_data_stacked)\n",
    "    embeddings_dict = {key: embeddings_stacked[i].detach().numpy() for i, key in enumerate(data_dict.keys())}\n",
    "    return embeddings_dict\n",
    "\n",
    "def embeddings_to_faiss_index(\n",
    "        embeddings_dict: Dict[str, np.ndarray], \n",
    "        index_type: str, \n",
    "        index_args: List[Any] = [], \n",
    "        index_kwargs: Dict[str, Any] = {}\n",
    "    ) -> CustomFAISSIndex:\n",
    "\n",
    "    embedding_index = CustomFAISSIndex(index_type=index_type, index_args=index_args, index_kwargs=index_kwargs)\n",
    "    for key, value in embeddings_dict.items():\n",
    "        embedding_index.add_with_id(key, value, data_dict[key])\n",
    "    return embedding_index\n",
    "\n",
    "    # For each embedding collection in embeddings_dicts, we perform the augmentation evaluation experiment num_loops times.\n",
    "    # An augmentation evaluation experiment involves the following steps:\n",
    "    # - Randomly select an anchor from data_dict\n",
    "    # - Remove the anchor from data_dict\n",
    "    # - Apply each of the five given augmentations to the anchor\n",
    "    # - For each of the augmented melodies, add it to the database and perform a nearest neighbor search on the FAISS index\n",
    "    # - Calculate the precision and recall of the search for each k in k_list\n",
    "\n",
    "def evaluate_top_k(\n",
    "        embedding_dict: Dict[str, Dict[str, np.ndarray]],\n",
    "        augment_dict: Dict[str, Dict[str, np.ndarray]], \n",
    "        k_list: List[int], \n",
    "        index: CustomFAISSIndex\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    index is a CustomFAISSIndex object which has been initialized with the embeddings_dict.\n",
    "    For each of the keys in augment_dict, we perform the following steps:\n",
    "    - Remove the anchor (embedding_dict[key]) from the index\n",
    "    - Add one of the augmentations from that key to the index\n",
    "    - Perform a nearest neighbor search on the index using the anchor and record the position of the augmentation\n",
    "    - Repeat for each augmentation\n",
    "    \n",
    "    Then we report the average precision and recall for each k in k_list.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_dict: dictionary of embeddings, keyed by data ids\n",
    "        augment_dict: dictionary keyed by a subset of the ids in embeddings_dict, containing dictionaries of augmented data\n",
    "        k_list: list of k values to evaluate\n",
    "        num_loops: number of loops to perform\n",
    "    \"\"\"\n",
    "    results = {aug_type: {k: {'precision': [], 'recall': []} for k in k_list} for aug_type in augment_dict[list(augment_dict.keys())[0]].keys()}\n",
    "    \n",
    "    for anchor_id, augmentations in augment_dict.items():\n",
    "        anchor_embedding = embedding_dict[anchor_id]\n",
    "        \n",
    "        # Remove anchor from index\n",
    "        index.remove(anchor_id)\n",
    "        \n",
    "        for aug_type, augmented_data in augmentations.items():\n",
    "            # Add augmented data to index\n",
    "            aug_id = f\"{anchor_id}_aug_{aug_type}\"\n",
    "            index.add_with_id(aug_id, augmented_data, original_data=augmented_data)\n",
    "            \n",
    "            # Perform search\n",
    "            search_results = index.search(anchor_embedding, max(k_list))\n",
    "            \n",
    "            # Calculate precision and recall for each k\n",
    "            for k in k_list:\n",
    "                top_k_results = search_results[:k]\n",
    "                true_positives = sum(1 for id, _ in top_k_results if id == aug_id)\n",
    "                \n",
    "                precision = true_positives / k\n",
    "                recall = 1 if true_positives > 0 else 0  # Recall is 1 if found, 0 if not\n",
    "                \n",
    "                results[aug_type][k]['precision'].append(precision)\n",
    "                results[aug_type][k]['recall'].append(recall)\n",
    "            \n",
    "            # Remove augmented data from index\n",
    "            index.remove(aug_id)\n",
    "        \n",
    "        # Add anchor back to index\n",
    "        index.add_with_id(anchor_id, anchor_embedding, original_data=index.get_original_data(anchor_id))\n",
    "    \n",
    "    # Calculate average precision and recall\n",
    "    for aug_type in results:\n",
    "        for k in k_list:\n",
    "            results[aug_type][k]['avg_precision'] = np.mean(results[aug_type][k]['precision'])\n",
    "            results[aug_type][k]['avg_recall'] = np.mean(results[aug_type][k]['recall'])\n",
    "    \n",
    "    return results\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/vectorstores/faiss/#similarity-search-with-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void __cdecl faiss::Index::add_with_ids(__int64,const float *,const __int64 *) at D:\\bld\\faiss-split_1723208824085\\work\\faiss\\Index.cpp:45: add_with_ids not implemented for this type of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_94768\\3404307021.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexFlatL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_with_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\cunn2\\miniconda3\\envs\\sms\\Lib\\site-packages\\faiss\\class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, ids)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'not same nb of vectors as ids'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_with_ids_c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\cunn2\\miniconda3\\envs\\sms\\Lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, n, x, xids)\u001b[0m\n\u001b[0;32m   1902\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m         \u001b[0minput\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1903\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mxids\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mxids\u001b[0m\u001b[1;33m:\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstore\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m         \"\"\"\n\u001b[1;32m-> 1906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndex_add_with_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in void __cdecl faiss::Index::add_with_ids(__int64,const float *,const __int64 *) at D:\\bld\\faiss-split_1723208824085\\work\\faiss\\Index.cpp:45: add_with_ids not implemented for this type of index"
     ]
    }
   ],
   "source": [
    "dshape = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dshape)\n",
    "\n",
    "ids = list(range(len(embeddings)))\n",
    "\n",
    "index.add_with_ids(embeddings.detach(), ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001CF344E2A90> >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.add_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Tuple\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "# Assuming you have these functions implemented\n",
    "from your_module import get_embedding, apply_augmentation\n",
    "\n",
    "def top_k_query(query_embedding: np.ndarray, database: List[np.ndarray], k: int) -> List[int]:\n",
    "    distances = [np.linalg.norm(query_embedding - db_embedding) for db_embedding in database]\n",
    "    return np.argsort(distances)[:k]\n",
    "\n",
    "def evaluate_top_k(dataset: List[np.ndarray], k: int, num_tests: int = 1000) -> Tuple[float, float]:\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    \n",
    "    augmentations = ['transpose', 'tempo_change', 'add_noise', 'remove_notes', 'add_notes', 'change_octave']\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        # Randomly select an anchor\n",
    "        anchor_idx = random.randint(0, len(dataset) - 1)\n",
    "        anchor = dataset[anchor_idx]\n",
    "        \n",
    "        # Remove anchor from dataset\n",
    "        reduced_dataset = dataset[:anchor_idx] + dataset[anchor_idx+1:]\n",
    "        \n",
    "        for aug in augmentations:\n",
    "            # Apply augmentation to anchor\n",
    "            augmented_anchor = apply_augmentation(anchor, aug)\n",
    "            \n",
    "            # Get embeddings\n",
    "            query_embedding = get_embedding(augmented_anchor)\n",
    "            database_embeddings = [get_embedding(chunk) for chunk in reduced_dataset]\n",
    "            \n",
    "            # Perform top-K query\n",
    "            top_k_results = top_k_query(query_embedding, database_embeddings, k)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            # Assuming the original anchor should be the only true positive\n",
    "            true_positives = 1 if anchor_idx in top_k_results else 0\n",
    "            precision = true_positives / k\n",
    "            recall = true_positives / 1  # Only one relevant item\n",
    "            \n",
    "            all_precisions.append(precision)\n",
    "            all_recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    \n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "# Load your dataset of melody chunks\n",
    "dataset = load_melody_chunks()  # Implement this function to load your dataset\n",
    "\n",
    "# Perform evaluation\n",
    "k_values = [1, 5, 10, 20]\n",
    "for k in k_values:\n",
    "    precision, recall = evaluate_top_k(dataset, k)\n",
    "    print(f\"Top-{k} Results:\")\n",
    "    print(f\"Average Precision: {precision:.4f}\")\n",
    "    print(f\"Average Recall: {recall:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation code plan:\n",
    "\n",
    "- faiss database:\n",
    "  - load val_data.pt\n",
    "  - get embeddings from given encoder\n",
    "  - store in database with way to retrieve index and remove/add vectors\n",
    "  \n",
    "- evlauation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
