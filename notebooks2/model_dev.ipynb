{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.chdir(\"c:/Users/cunn2/OneDrive/DSML/Project/thesis-repo\")\n",
    "\n",
    "from sms.exp1.config_classes import load_config_from_launchplan\n",
    "from sms.exp1.run_training import build_encoder, build_projector\n",
    "from sms.exp1.models.siamese import SiameseModel\n",
    "\n",
    "# config = load_config_from_launchplan(\"sms/exp1/runs/run_20240926_162652/original_launchplan.yaml\")\n",
    "\n",
    "# encoder = build_encoder(config.model_dump())\n",
    "# projector = build_projector(config.model_dump())\n",
    "\n",
    "# model = SiameseModel(encoder, projector)\n",
    "\n",
    "# print(encoder)\n",
    "# print(projector)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_469720\\3167217028.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\data\\exp1\\train_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "# bert\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class TokenAndPositionalEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.token_emb = nn.Conv1d(self.input_dim, self.emb_dim, 1)\n",
    "        self.pos_emb = self.positional_encoding(self.max_len, self.emb_dim)\n",
    "\n",
    "    def get_angles(self, pos, i, emb_dim):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(emb_dim))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, emb_dim):\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(emb_dim)[np.newaxis, :],\n",
    "            emb_dim,\n",
    "        )\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x = self.token_emb(x)\n",
    "        x *= torch.sqrt(torch.tensor(self.emb_dim, dtype=torch.float32))\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        return x + self.pos_emb.to(x.device)[:, : x.shape[1]]\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config, input_shape=2, d_latent=64):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.d_input = input_shape\n",
    "        self.d_latent = d_latent\n",
    "        self.d_model = config.get(\"d_model\", 128)\n",
    "        self.n_layers = config.get(\"n_layers\", 4)\n",
    "\n",
    "        self.emb = TokenAndPositionalEmbeddingLayer(\n",
    "            input_dim=self.d_input, emb_dim=self.d_model, max_len=config.get(\"max_seq_len\", 512)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=config.get(\"n_heads\", 8),\n",
    "            dim_feedforward=config.get(\"d_ff\", self.d_model * 4),\n",
    "            dropout=config.get(\"dropout_rate\", 0.1),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=self.n_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.d_model, self.d_latent)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # (assuming input batch has shape [batch_size, padded_seq_length, point_dim])\n",
    "        # batch_key_padding_mask are all False, so the output is the same as batch. This is because all inputs have the same length.\n",
    "        batch_key_padding_mask = torch.zeros((batch.shape[0], batch.shape[1])).bool()\n",
    "        batch_key_padding_mask = batch_key_padding_mask.to(batch.device)\n",
    "        batch_emb = self.emb(batch)             # (batch_size, padded_seq_length, d_model)\n",
    "        batch_emb = self.transformer_encoder(\n",
    "            batch_emb, batch_key_padding_mask=batch_key_padding_mask\n",
    "        )                                       # (batch_size, padded_seq_length, d_model)\n",
    "        batch_emb = self.fc(batch_emb)          # (batch_size, padded_seq_length, d_latent)\n",
    "        batch_emb = torch.permute(batch_emb, (0, 2, 1))  # (batch_size, d_latent, padded_seq_length)\n",
    "        batch_emb = self.pool(batch_emb)            # (batch_size, d_latent, 1)\n",
    "        batch_emb = torch.squeeze(batch_emb, dim=2)  # (batch_size, d_latent)\n",
    "\n",
    "        return batch_emb\n",
    "    \n",
    "data = torch.load(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\data\\exp1\\train_data.pt\")\n",
    "max_length = max([len(chunk) for chunk in data])\n",
    "print(max_length)\n",
    "\n",
    "dumped_lp_config = {\n",
    "    \"encoder\": {\n",
    "        \"type\": \"BertEncoder\",\n",
    "        \"params\": {\n",
    "            \"config\": {\n",
    "                \"d_model\": 128,\n",
    "                \"n_layers\": 4,\n",
    "                \"n_heads\": 8,\n",
    "                \"d_ff\": 512,\n",
    "                \"d_expander\": 256,\n",
    "                \"dropout_rate\": 0.1,\n",
    "                \"max_seq_len\": 512\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"dims\": {\n",
    "        \"input_shape\": 2,\n",
    "        \"d_latent\": 64\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"make_relative_pitch\": True,\n",
    "        \"normalize_octave\": False,\n",
    "        \"piano_roll\": False,\n",
    "        \"quantize\": False,\n",
    "        \"rest_pitch\": -1,\n",
    "        \"steps_per_bar\": 32,\n",
    "        \"pad_sequence\": True,\n",
    "        \"pad_val\": -1000,\n",
    "        \"goal_seq_len\": max_length\n",
    "    }\n",
    "}\n",
    "\n",
    "encoder = build_encoder(dumped_lp_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (emb): TokenAndPositionalEmbeddingLayer(\n",
       "    (token_emb): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sms.src.synthetic_data.formatter import InputFormatter\n",
    "\n",
    "data_ex = data[:10]\n",
    "\n",
    "formatter = InputFormatter(**dumped_lp_config['input'])\n",
    "formatted_data_list = [torch.from_numpy(formatter(chunk).astype(np.float32).copy()) for chunk in data_ex]\n",
    "formatted_data_stacked = torch.stack(formatted_data_list, dim=0) # shape [num_chunks, *input_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1694,  0.6252, -1.2926,  0.3294,  0.7557,  0.0592,  0.5362,  0.8396,\n",
       "         -0.2769,  0.7450, -0.0067,  0.3244,  0.6042,  0.5901,  0.2478,  0.5685,\n",
       "          0.0820, -0.6198, -0.8019, -0.3445,  1.0504, -0.4134, -0.6586,  0.0849,\n",
       "          0.4765, -0.1593,  0.8542,  0.3546,  0.7130,  0.0470,  0.3648,  0.6061,\n",
       "          0.0992,  0.2861, -0.2881,  0.5077,  0.5903,  0.0130, -0.3429, -0.5028,\n",
       "          0.9624, -0.2409,  0.0261, -0.1905, -0.3776, -0.0946, -0.7997,  0.4746,\n",
       "         -0.1005, -0.2990,  0.5326,  0.4430,  0.1011, -1.3552, -0.4408, -0.2314,\n",
       "         -0.8058,  0.0802, -0.3196, -0.3843, -0.7096,  0.5022, -0.1368, -0.1357],\n",
       "        [-0.2532,  0.4248, -0.8243,  0.2140,  0.5478,  0.1332,  0.4364,  0.5299,\n",
       "         -0.4181,  0.6948, -0.2095,  0.2311,  0.8965,  0.6979,  0.3164,  0.4584,\n",
       "         -0.0208, -0.6818, -0.6604, -0.2539,  0.8566, -0.4693, -0.6859,  0.0717,\n",
       "          0.3832,  0.0359,  0.8730,  0.3418,  0.4909,  0.1239,  0.3962,  0.7210,\n",
       "          0.3641,  0.3725,  0.0590,  0.4555,  0.4633,  0.0087, -0.4987, -0.1407,\n",
       "          0.9033, -0.2071,  0.0087, -0.3663, -0.2577, -0.1793, -0.7729,  0.2724,\n",
       "          0.0662, -0.1118,  0.5496,  0.3530,  0.2027, -1.0382, -0.5985, -0.3702,\n",
       "         -0.5360,  0.0974, -0.3496, -0.4099, -0.6830,  0.6877, -0.5213, -0.2179],\n",
       "        [-0.1132,  0.6884, -1.0636,  0.1208,  0.5675,  0.1340,  0.6497,  0.7248,\n",
       "         -0.4582,  0.8927, -0.0856,  0.3752,  0.9788,  0.6479,  0.1178,  0.6623,\n",
       "         -0.0592, -0.7172, -0.7120, -0.2787,  0.9081, -0.3449, -0.9781,  0.0574,\n",
       "          0.5741, -0.0373,  1.0531,  0.2182,  0.5365,  0.0246,  0.4395,  0.6656,\n",
       "          0.3412,  0.4177,  0.1520,  0.4872,  0.6063, -0.0824, -0.2675, -0.3772,\n",
       "          1.0723, -0.2316,  0.0596, -0.2986, -0.2069, -0.1418, -0.8130,  0.4814,\n",
       "         -0.0902, -0.3163,  0.5878,  0.3516,  0.1808, -1.2092, -0.6600, -0.3211,\n",
       "         -0.7214, -0.0035, -0.3237, -0.2960, -0.7765,  0.6473, -0.3287, -0.1261],\n",
       "        [-0.2049,  0.4903, -0.9237,  0.2452,  0.5829,  0.0372,  0.4740,  0.7496,\n",
       "         -0.3914,  0.7658, -0.0459,  0.2580,  0.8635,  0.5818,  0.1101,  0.4257,\n",
       "          0.0334, -0.8885, -0.7455, -0.3363,  0.7928, -0.4583, -0.8351, -0.0053,\n",
       "          0.4598,  0.1961,  1.0108,  0.3372,  0.4623,  0.0484,  0.3386,  0.6762,\n",
       "          0.4026,  0.4000,  0.1457,  0.6107,  0.6107, -0.0240, -0.3402, -0.0932,\n",
       "          0.7224, -0.2212, -0.0020, -0.3128, -0.2458, -0.2963, -0.6936,  0.2574,\n",
       "         -0.0305, -0.1982,  0.7463,  0.4996,  0.0948, -1.1352, -0.4714, -0.3761,\n",
       "         -0.6627,  0.1561, -0.1714, -0.3341, -0.7157,  0.4191, -0.4457, -0.2218],\n",
       "        [-0.0338,  0.3178, -0.9231,  0.1772,  0.4700,  0.2265,  0.6462,  0.6751,\n",
       "         -0.6974,  1.0898, -0.2607,  0.3581,  0.8187,  0.5279, -0.0509,  0.3726,\n",
       "         -0.0971, -0.5538, -0.3974, -0.4537,  0.7488, -0.5456, -0.6331,  0.1214,\n",
       "          0.6606,  0.2058,  1.0102,  0.3362,  0.5900,  0.0988,  0.3679,  0.7150,\n",
       "          0.3630,  0.5217,  0.1794,  0.5375,  0.5094, -0.0268, -0.2936, -0.3552,\n",
       "          0.9327, -0.0450,  0.1197, -0.4363, -0.3824, -0.2459, -0.8015,  0.5424,\n",
       "         -0.0851, -0.2550,  0.5884,  0.3160,  0.2593, -0.9045, -0.6409, -0.2897,\n",
       "         -0.6254,  0.0356, -0.3344, -0.4886, -0.8624,  0.5835, -0.3855,  0.0718],\n",
       "        [-0.1093,  0.3809, -0.9383,  0.2156,  0.4725,  0.0374,  0.6365,  0.7023,\n",
       "         -0.5669,  0.8762, -0.1199,  0.1021,  0.8433,  0.5984,  0.1973,  0.3011,\n",
       "          0.1542, -0.7870, -0.5912, -0.3179,  0.6293, -0.3968, -0.8573,  0.1887,\n",
       "          0.4685,  0.3176,  0.9600,  0.4078,  0.4304, -0.0054,  0.3580,  0.8541,\n",
       "          0.5039,  0.4027,  0.2693,  0.6100,  0.4986, -0.0419, -0.3379, -0.1689,\n",
       "          0.8787, -0.1876,  0.1277, -0.3807, -0.3394, -0.3087, -0.6008,  0.6111,\n",
       "         -0.0149, -0.2990,  0.5673,  0.3683,  0.3503, -0.9898, -0.7016, -0.4528,\n",
       "         -0.6691,  0.1933, -0.1837, -0.4678, -0.7663,  0.5684, -0.6563,  0.0121],\n",
       "        [-0.1324,  0.3253, -1.0018,  0.1095,  0.4847,  0.0778,  0.6632,  0.6489,\n",
       "         -0.6981,  1.1264, -0.1837,  0.3215,  0.8362,  0.6045,  0.0484,  0.3497,\n",
       "         -0.1074, -0.7417, -0.4669, -0.2977,  0.7909, -0.6133, -0.8782,  0.0968,\n",
       "          0.4477,  0.2596,  1.1056,  0.1531,  0.6067,  0.0284,  0.2733,  0.6699,\n",
       "          0.4219,  0.4734,  0.2287,  0.3821,  0.6017,  0.0541, -0.2942, -0.3325,\n",
       "          0.9437, -0.1366,  0.1455, -0.4927, -0.1659, -0.1427, -0.9283,  0.4308,\n",
       "         -0.2288, -0.3462,  0.6399,  0.3569,  0.1268, -0.9285, -0.5690, -0.2266,\n",
       "         -0.7434, -0.0726, -0.2766, -0.4194, -0.8037,  0.6207, -0.4607,  0.0023],\n",
       "        [-0.2963,  0.4597, -1.3574,  0.2605,  0.6139,  0.0644,  0.3648,  0.5334,\n",
       "         -0.5485,  0.7909, -0.1408,  0.4221,  0.5700,  0.5390,  0.1414,  0.7198,\n",
       "          0.1389, -0.8523, -0.7074, -0.2560,  0.8861, -0.5081, -0.6998, -0.0193,\n",
       "          0.4564,  0.0180,  0.8808,  0.3429,  0.5618,  0.2749,  0.4267,  0.6073,\n",
       "          0.2560,  0.4911, -0.1530,  0.4163,  0.6320,  0.1083, -0.3746, -0.4446,\n",
       "          0.8701, -0.2054,  0.2611, -0.2494, -0.0902, -0.2515, -0.7684,  0.2649,\n",
       "         -0.1292, -0.1997,  0.7266,  0.4049,  0.1069, -1.2000, -0.4860, -0.3505,\n",
       "         -0.4677,  0.0456, -0.2487, -0.3308, -0.7336,  0.6462, -0.3489, -0.1071],\n",
       "        [-0.1478,  0.3170, -0.9935,  0.1844,  0.4211,  0.1360,  0.5580,  0.4887,\n",
       "         -0.6663,  0.8862, -0.2513,  0.3128,  0.8821,  0.6491,  0.1809,  0.4362,\n",
       "          0.0955, -0.7036, -0.6205, -0.2383,  0.7464, -0.5395, -0.7736,  0.1264,\n",
       "          0.4382,  0.1912,  1.0183,  0.2743,  0.5776,  0.1251,  0.2951,  0.7470,\n",
       "          0.3743,  0.4446,  0.1690,  0.3886,  0.5368, -0.0157, -0.3153, -0.2427,\n",
       "          0.8735, -0.1599,  0.2364, -0.4626, -0.1253, -0.1890, -0.8651,  0.3399,\n",
       "         -0.1485, -0.2356,  0.6073,  0.4077,  0.2363, -0.9843, -0.6724, -0.3286,\n",
       "         -0.5783,  0.1673, -0.1611, -0.3806, -0.7036,  0.6508, -0.6027, -0.0076],\n",
       "        [ 0.1179,  0.6021, -0.8316,  0.1511,  0.4445,  0.3229,  0.6136,  0.8511,\n",
       "         -0.4678,  1.1931, -0.0704,  0.0565,  0.8013,  0.7952,  0.0449,  0.2211,\n",
       "          0.2095, -0.8060, -0.3818, -0.2502,  0.6014, -0.4698, -1.0390,  0.0082,\n",
       "          0.3685,  0.4556,  0.9708,  0.3244,  0.4226, -0.2031,  0.2555,  0.8890,\n",
       "          0.5249,  0.4741,  0.2849,  0.6727,  0.5108, -0.1102, -0.2877, -0.1119,\n",
       "          0.7358, -0.1401,  0.0733, -0.4585, -0.6173, -0.2518, -0.6965,  0.6493,\n",
       "         -0.3417, -0.5086,  0.4023,  0.2080,  0.4101, -0.8400, -0.7492, -0.1598,\n",
       "         -0.6932, -0.1252, -0.0301, -0.4710, -0.8311,  0.4621, -0.6843, -0.0042]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(formatted_data_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2 67. ]\n",
      " [ 1.  74. ]\n",
      " [ 2.  76. ]\n",
      " [ 0.8 74. ]]\n",
      "[[ 2.0e-01  6.7e+01]\n",
      " [ 1.0e+00  7.4e+01]\n",
      " [ 2.0e+00  7.6e+01]\n",
      " [ 8.0e-01  7.4e+01]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]]\n"
     ]
    }
   ],
   "source": [
    "formatter = InputFormatter(pad_sequence=True)\n",
    "\n",
    "print(data[0])\n",
    "print(formatter(data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(sum(torch.tensor([5,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  0,  0,  1,  4,  5,  5,  7,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10])\n",
      "True\n",
      "torch.Size([10, 38, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2980, -0.1535, -0.1493,  ..., -0.2699, -0.0711, -0.3754],\n",
       "        [ 0.4129,  0.0276, -0.0893,  ..., -0.2565,  0.0164, -0.1155],\n",
       "        [ 0.1244,  0.1676, -0.2458,  ..., -0.0757, -0.0278, -0.1218],\n",
       "        ...,\n",
       "        [ 0.3211, -0.0522, -0.1985,  ..., -0.1279, -0.1014, -0.0584],\n",
       "        [ 0.3891,  0.1941, -0.0784,  ..., -0.0367,  0.1587, -0.3743],\n",
       "        [ 0.1002,  0.0830, -0.1873,  ...,  0.1531,  0.1510, -0.0222]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config_from_launchplan(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\sms\\exp1\\launchplans\\transformer_rel.yaml\")\n",
    "\n",
    "dumped_lp_config = config.model_dump()\n",
    "encoder = build_encoder(dumped_lp_config)\n",
    "\n",
    "data_ex = data[:10]\n",
    "\n",
    "formatter = InputFormatter(**dumped_lp_config['input'])\n",
    "formatted_data_list = [torch.from_numpy(formatter(chunk).astype(np.float32).copy()) for chunk in data_ex]\n",
    "formatted_data_stacked = torch.stack(formatted_data_list, dim=0) # shape [num_chunks, *input_shape]\n",
    "\n",
    "print(sum(torch.all(formatted_data_stacked == -1000, dim=-1)))\n",
    "print(bool(torch.sum(torch.all(formatted_data_stacked == -1000, dim=-1))))\n",
    "print(formatted_data_stacked.shape)\n",
    "\n",
    "encoder(formatted_data_stacked)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
