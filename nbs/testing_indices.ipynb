{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_84612\\2905634211.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(r\"data\\exp1\\val_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\")\n",
    "\n",
    "from sms.src.log import configure_logging\n",
    "from sms.src.vector_search.evaluate_top_k import create_augmented_data, build_model, create_embedding_dict, embeddings_to_faiss_index, evaluate_top_k\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from sms.exp1.config_classes import LaunchPlanConfig, load_config_from_launchplan\n",
    "from sms.exp1.run_evaluation import run_evaluation, ModelEvalConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "configure_logging(console_level=logging.INFO)\n",
    "\n",
    "data = torch.load(r\"data\\exp1\\val_data.pt\")\n",
    "data_ids = [str(uuid4()) for _ in range(len(data))]\n",
    "data_dict = dict(zip(data_ids, data))\n",
    "\n",
    "class IndexConfig(BaseModel):\n",
    "    index_type: str\n",
    "    index_args: List[Any] = []\n",
    "    index_kwargs: Dict[str, Any] = {}\n",
    "\n",
    "trans_rel_lp_cfg = load_config_from_launchplan(r\"sms\\exp1\\runs\\transformer_rel_1\\original_launchplan.yaml\")\n",
    "\n",
    "trans_rel_1_full = ModelEvalConfig(\n",
    "    name=\"trans_rel_1_full\",\n",
    "    lp_config=trans_rel_lp_cfg,\n",
    "    mod_path=r\"sms\\exp1\\runs\\transformer_rel_1\\pretrain_saved_model.pth\",\n",
    "    path_type='full',\n",
    "    use_full_model=True\n",
    ")\n",
    "\n",
    "dim = trans_rel_lp_cfg.model_dump()['dims']['d_projected']\n",
    "\n",
    "idx_cfg = IndexConfig(index_type=\"IndexLSH\", index_args=[dim, 32])\n",
    "\n",
    "def run_evaluation(\n",
    "    data_dict: Dict[str, np.ndarray],\n",
    "    num_loops: int,\n",
    "    model_configs: List[ModelEvalConfig],\n",
    "    index_config: IndexConfig\n",
    "    ) -> Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]]:\n",
    "\n",
    "    # generate random augmentations\n",
    "    anchor_keys = np.random.choice(list(data_dict.keys()), size=num_loops, replace=False)\n",
    "    augmented_data = create_augmented_data(data_dict, anchor_keys)\n",
    "\n",
    "    results = {}\n",
    "    for eval_config in model_configs:\n",
    "        logger.info(f\"Running evaluation for {eval_config.name}\")\n",
    "\n",
    "        dumped_lp_config = eval_config.lp_config.model_dump()\n",
    "        bm_cfg = {'full_model_path': eval_config.mod_path} if eval_config.path_type == 'full' else {'encoder_path': eval_config.mod_path}\n",
    "\n",
    "        model = build_model(dumped_lp_config, **bm_cfg, use_full_model=eval_config.use_full_model)\n",
    "        embeddings_dict = create_embedding_dict(data_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created embedding dictionary for {len(embeddings_dict)} keys.\")\n",
    "        # create augmented embeddings structure\n",
    "        augmented_embeddings_dict = {}\n",
    "        for data_id, aug_dict in augmented_data.items():\n",
    "            augmented_embeddings_dict[data_id] = create_embedding_dict(aug_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created augmented embeddings.\")\n",
    "\n",
    "        index = embeddings_to_faiss_index(embeddings_dict=embeddings_dict, **index_config.model_dump())\n",
    "        logger.info(f\"Created FAISS index.\")\n",
    "        \n",
    "        results[eval_config.name] = evaluate_top_k(embeddings_dict, augmented_embeddings_dict, [1, 3, 5, 10, 25, 50, 100], index)\n",
    "        logger.info(f\"Evaluated top K.\")\n",
    "    return results\n",
    "\n",
    "# results = run_evaluation(data_dict, 100, [trans_rel_1_full], idx_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing searches on the indices we want to use\n",
    "\n",
    "we want to use\n",
    "\n",
    "IVF, PQ, HNSW, LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexConfig(BaseModel):\n",
    "    index_type: str\n",
    "    index_args: List[Any] = []\n",
    "    index_kwargs: Dict[str, Any] = {}\n",
    "\n",
    "trans_rel_lp_cfg = load_config_from_launchplan(r\"sms\\exp1\\runs\\transformer_rel_1\\original_launchplan.yaml\")\n",
    "\n",
    "trans_rel_1_full = ModelEvalConfig(\n",
    "    name=\"trans_rel_1_full\",\n",
    "    lp_config=trans_rel_lp_cfg,\n",
    "    mod_path=r\"sms\\exp1\\runs\\transformer_rel_1\\pretrain_saved_model.pth\",\n",
    "    path_type='full',\n",
    "    use_full_model=True\n",
    ")\n",
    "\n",
    "model = build_model(trans_rel_lp_cfg.model_dump(), full_model_path=r\"sms\\exp1\\runs\\transformer_rel_1\\pretrain_saved_model.pth\", use_full_model=trans_rel_1_full.use_full_model)\n",
    "embeddings_dict = create_embedding_dict(data_dict, trans_rel_lp_cfg.model_dump(), model)\n",
    "idx = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexFlatL2\", index_args=[dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "dim = list(embeddings_dict.values())[0].shape[0]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_262856\\3892824433.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ds = torch.load(r\"data/exp2/million_chunks.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple, List, Dict\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.chdir(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\")\n",
    "\n",
    "def select_keys(data_dict: Dict[str, np.ndarray], shuffle: bool = True) -> Tuple[Dict[str, List[str]], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Shuffle the keys and select the required number of keys for each dataset size.\n",
    "    \n",
    "    Returns two dictionaries:\n",
    "    - subset_keys: Maps dataset sizes to their subset of keys.\n",
    "    - selected_keys: Maps dataset sizes to their selected keys for augmentation.\n",
    "    \"\"\"\n",
    "    keys = list(data_dict.keys())\n",
    "    if shuffle:\n",
    "        np.random.shuffle(keys)\n",
    "        logger.info(\"Shuffled the dataset keys.\")\n",
    "    \n",
    "    selection_plan = [\n",
    "        (5000, 100, '5k'),\n",
    "        (10000, 200, '10k'),\n",
    "        (100000, 1000, '100k'),\n",
    "        (500000, 2000, '500k'),\n",
    "        (None, 10000, '1m') \n",
    "    ]\n",
    "\n",
    "    subset_keys = {}\n",
    "    selected_keys = {}\n",
    "    start = 0\n",
    "    total_keys = len(keys)\n",
    "\n",
    "    for size, count, label in selection_plan:\n",
    "        if size is not None:\n",
    "            end = start + size\n",
    "            subset = keys[start:end]\n",
    "            subset_keys[label] = subset\n",
    "            selected = subset[:count]\n",
    "            selected_keys[label] = selected\n",
    "            print(f\"Selected {len(selected)} keys for dataset size {label} from subset of {len(subset)} keys.\")\n",
    "            start = end\n",
    "            if start >= total_keys:\n",
    "                logger.warning(f\"Reached the end of the dataset at size {label}.\")\n",
    "                break\n",
    "        else:\n",
    "            # For 1m, select from the remaining keys\n",
    "            selected = keys[start:start + count]\n",
    "            selected_keys[label] = selected\n",
    "            subset_keys[label] = keys\n",
    "            logger.info(f\"Selected {len(selected)} keys for dataset size {label} from the entire remaining dataset.\")\n",
    "    \n",
    "    return subset_keys, selected_keys\n",
    "\n",
    "ds = torch.load(r\"data/exp2/million_chunks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(ds.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([len(val) for val in vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18639"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lengths<3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the lengths of each item in vals\n",
    "lengths = [len(item) for item in vals]\n",
    "\n",
    "# Calculate basic statistics\n",
    "min_length = np.min(lengths)\n",
    "max_length = np.max(lengths)\n",
    "mean_length = np.mean(lengths)\n",
    "median_length = np.median(lengths)\n",
    "\n",
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Lengths in vals')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add vertical lines for mean and median\n",
    "plt.axvline(mean_length, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean_length:.2f}')\n",
    "plt.axvline(median_length, color='g', linestyle='dashed', linewidth=2, label=f'Median: {median_length:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Add text box with statistics\n",
    "stats_text = f'Min: {min_length}\\nMax: {max_length}\\nMean: {mean_length:.2f}\\nMedian: {median_length:.2f}'\n",
    "plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, verticalalignment='top', horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum length: {min_length}\")\n",
    "print(f\"Maximum length: {max_length}\")\n",
    "print(f\"Mean length: {mean_length:.2f}\")\n",
    "print(f\"Median length: {median_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 keys for dataset size 5k from subset of 5000 keys.\n",
      "Selected 200 keys for dataset size 10k from subset of 10000 keys.\n",
      "Selected 1000 keys for dataset size 100k from subset of 100000 keys.\n",
      "Selected 2000 keys for dataset size 500k from subset of 500000 keys.\n"
     ]
    }
   ],
   "source": [
    "a, b = select_keys(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b['5k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# flat\n",
    "idx_flat = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexFlatL2\", index_args=[dim])\n",
    "# IVF\n",
    "quantizer = faiss.IndexFlatL2(dim)\n",
    "nlist = np.sqrt(len(embeddings_dict))\n",
    "idx_IVF = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexIVFFlat\", index_args=[quantizer, int(dim), int(nlist)])\n",
    "# PQ (cant do radius search)\n",
    "M = 8\n",
    "nbits = 8\n",
    "idx_PQ = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexPQ\", index_args=[dim, M, nbits]) \n",
    "# HNSW\n",
    "M = 32\n",
    "idx_HNSW = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexHNSWFlat\", index_args=[dim, M])\n",
    "# LSH (cant do radius search)\n",
    "nbits = 64\n",
    "idx_LSH = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexLSH\", index_args=[dim, nbits])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80692863\n"
     ]
    }
   ],
   "source": [
    "v1 = list(embeddings_dict.values())[0]\n",
    "v2 = list(embeddings_dict.values())[5]\n",
    "i2 = list(embeddings_dict.keys())[5]\n",
    "\n",
    "diff = np.linalg.norm(v1 - v2)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             data_path num_loops model_config_paths\n",
      "                             [model_config_paths ...] output_path\n",
      "ipykernel_launcher.py: error: the following arguments are required: data_path, num_loops, model_config_paths, output_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from sms.src.log import configure_logging\n",
    "from sms.src.vector_search.evaluate_top_k import create_augmented_data, build_model, create_embedding_dict, embeddings_to_faiss_index, evaluate_top_k\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from sms.exp1.config_classes import LaunchPlanConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "configure_logging()\n",
    "\n",
    "class ModelEvalConfig(BaseModel):\n",
    "    name: str\n",
    "    lp_config: LaunchPlanConfig\n",
    "    mod_path: str\n",
    "    path_type: str    #'full' or 'encoder'\n",
    "    use_full_model: bool\n",
    "\n",
    "class IndexConfig(BaseModel):\n",
    "    index_type: str\n",
    "    index_args: List[Any] = []\n",
    "    index_kwargs: Dict[str, Any] = {}\n",
    "\n",
    "def run_evaluation(\n",
    "    data_dict: Dict[str, np.ndarray],\n",
    "    num_loops: int,\n",
    "    model_configs: List[ModelEvalConfig],\n",
    "    index_configs: List[IndexConfig]\n",
    "    ) -> Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]]:\n",
    "\n",
    "    \"\"\"\n",
    "    An extension of the evaluation code in exp1.\n",
    "    Runs topK evaluation for each model config and each index config.\n",
    "\n",
    "    Returns a results dictionary, which has the following structure:\n",
    "    {\n",
    "        model_name: {\n",
    "            index_name: {\n",
    "                topK: {\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1': []\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        data_dict: dictionary of data, where each value is a numpy array.\n",
    "        num_loops: number of loops to run.\n",
    "        model_configs: list of model configurations.\n",
    "        index_configs: list of index configurations.\n",
    "\n",
    "    Returns:\n",
    "        results: dictionary of results, where each value is a dictionary of topK evaluation results.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate random augmentations\n",
    "    anchor_keys = np.random.choice(list(data_dict.keys()), size=num_loops, replace=False)\n",
    "    augmented_data = create_augmented_data(data_dict, anchor_keys)\n",
    "\n",
    "    results = {}\n",
    "    for eval_config in model_configs:\n",
    "        logger.info(f\"Running evaluation for {eval_config.name}\")\n",
    "\n",
    "        dumped_lp_config = eval_config.lp_config.model_dump()\n",
    "        bm_cfg = {'full_model_path': eval_config.mod_path} if eval_config.path_type == 'full' else {'encoder_path': eval_config.mod_path}\n",
    "\n",
    "        model = build_model(dumped_lp_config, **bm_cfg, use_full_model=eval_config.use_full_model)\n",
    "        embeddings_dict = create_embedding_dict(data_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created embedding dictionary for {len(embeddings_dict)} keys.\")\n",
    "\n",
    "        # create augmented embeddings structure\n",
    "        augmented_embeddings_dict = {}\n",
    "        for data_id, aug_dict in augmented_data.items():\n",
    "            augmented_embeddings_dict[data_id] = create_embedding_dict(aug_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created augmented embeddings.\")\n",
    "\n",
    "        dim = list(embeddings_dict.values())[0].shape[0]\n",
    "\n",
    "        #TODO: record embedding dimension\n",
    "\n",
    "        # FLATL2 baseline \n",
    "        index_config = IndexConfig(index_type=\"IndexFlatL2\", index_args=[dim])\n",
    "        index = embeddings_to_faiss_index(embeddings_dict=embeddings_dict, **index_config.model_dump())\n",
    "        logger.info(f\"Created FAISS index with parameters {index_config.model_dump()}\")\n",
    "        results[eval_config.name] = evaluate_top_k(embeddings_dict, augmented_embeddings_dict, [1, 3, 5, 10, 25, 50, 100], index)\n",
    "        logger.info(f\"Evaluated top K.\")\n",
    "        #TODO: add to CustomFAISSINdex the details of the index, like bytes used for each embedding, databse memory usage.\n",
    "\n",
    "        #TODO: make sure timings are recorded\n",
    "        for index_config in index_configs:\n",
    "            index_config_dict = index_config.model_dump()\n",
    "            index = embeddings_to_faiss_index(embeddings_dict=embeddings_dict, **index_config_dict)\n",
    "            logger.info(f\"Created FAISS index with parameters {index_config_dict}\")\n",
    "            results[eval_config.name] = evaluate_top_k(embeddings_dict, augmented_embeddings_dict, [1, 3, 5, 10, 25, 50, 100], index)\n",
    "            logger.info(f\"Evaluated top K.\")\n",
    "    return results\n",
    "\n",
    "def main(data_path: str, num_loops: int, model_config_paths: List[str], output_path: str):\n",
    "    data_dict = pkl.load(open(data_path, 'rb'))\n",
    "    model_configs = []\n",
    "    for config_path in model_config_paths:\n",
    "        with open(config_path, 'r') as file:\n",
    "            config_data = yaml.safe_load(file)\n",
    "        try:\n",
    "            model_config = ModelEvalConfig(**config_data)\n",
    "            model_configs.append(model_config)\n",
    "        except pydantic.ValidationError as e:\n",
    "            logger.error(f\"Invalid configuration in {config_path}: {e}\")\n",
    "            raise\n",
    "    results = run_evaluation(data_dict, num_loops, model_configs)\n",
    "    pkl.dump(results, open(output_path, 'wb'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run model evaluation.\")\n",
    "    parser.add_argument('data_path', type=str, help='Path to the data file.')\n",
    "    parser.add_argument('num_loops', type=int, help='Number of loops for evaluation.')\n",
    "    parser.add_argument('model_config_paths', type=str, nargs='+', help='Paths to model configuration files.')\n",
    "    parser.add_argument('output_path', type=str, help='Path to the output file.')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args.data_path, args.num_loops, args.model_config_paths, args.output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
