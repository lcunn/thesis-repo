{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_84612\\2905634211.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(r\"data\\exp1\\val_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\")\n",
    "\n",
    "from sms.src.log import configure_logging\n",
    "from sms.src.vector_search.evaluate_top_k import create_augmented_data, build_model, create_embedding_dict, embeddings_to_faiss_index, evaluate_top_k\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from sms.exp1.config_classes import LaunchPlanConfig, load_config_from_launchplan\n",
    "from sms.exp1.run_evaluation import run_evaluation, ModelEvalConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "configure_logging(console_level=logging.INFO)\n",
    "\n",
    "data = torch.load(r\"data\\exp1\\val_data.pt\")\n",
    "data_ids = [str(uuid4()) for _ in range(len(data))]\n",
    "data_dict = dict(zip(data_ids, data))\n",
    "\n",
    "class IndexConfig(BaseModel):\n",
    "    index_type: str\n",
    "    index_args: List[Any] = []\n",
    "    index_kwargs: Dict[str, Any] = {}\n",
    "\n",
    "trans_rel_lp_cfg = load_config_from_launchplan(r\"sms\\exp1\\runs\\transformer_rel_1\\original_launchplan.yaml\")\n",
    "\n",
    "trans_rel_1_full = ModelEvalConfig(\n",
    "    name=\"trans_rel_1_full\",\n",
    "    lp_config=trans_rel_lp_cfg,\n",
    "    mod_path=r\"sms\\exp1\\runs\\transformer_rel_1\\pretrain_saved_model.pth\",\n",
    "    path_type='full',\n",
    "    use_full_model=True\n",
    ")\n",
    "\n",
    "dim = trans_rel_lp_cfg.model_dump()['dims']['d_projected']\n",
    "\n",
    "idx_cfg = IndexConfig(index_type=\"IndexLSH\", index_args=[dim, 32])\n",
    "\n",
    "def run_evaluation(\n",
    "    data_dict: Dict[str, np.ndarray],\n",
    "    num_loops: int,\n",
    "    model_configs: List[ModelEvalConfig],\n",
    "    index_config: IndexConfig\n",
    "    ) -> Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]]:\n",
    "\n",
    "    # generate random augmentations\n",
    "    anchor_keys = np.random.choice(list(data_dict.keys()), size=num_loops, replace=False)\n",
    "    augmented_data = create_augmented_data(data_dict, anchor_keys)\n",
    "\n",
    "    results = {}\n",
    "    for eval_config in model_configs:\n",
    "        logger.info(f\"Running evaluation for {eval_config.name}\")\n",
    "\n",
    "        dumped_lp_config = eval_config.lp_config.model_dump()\n",
    "        bm_cfg = {'full_model_path': eval_config.mod_path} if eval_config.path_type == 'full' else {'encoder_path': eval_config.mod_path}\n",
    "\n",
    "        model = build_model(dumped_lp_config, **bm_cfg, use_full_model=eval_config.use_full_model)\n",
    "        embeddings_dict = create_embedding_dict(data_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created embedding dictionary for {len(embeddings_dict)} keys.\")\n",
    "        # create augmented embeddings structure\n",
    "        augmented_embeddings_dict = {}\n",
    "        for data_id, aug_dict in augmented_data.items():\n",
    "            augmented_embeddings_dict[data_id] = create_embedding_dict(aug_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created augmented embeddings.\")\n",
    "\n",
    "        index = embeddings_to_faiss_index(embeddings_dict=embeddings_dict, **index_config.model_dump())\n",
    "        logger.info(f\"Created FAISS index.\")\n",
    "        \n",
    "        results[eval_config.name] = evaluate_top_k(embeddings_dict, augmented_embeddings_dict, [1, 3, 5, 10, 25, 50, 100], index)\n",
    "        logger.info(f\"Evaluated top K.\")\n",
    "    return results\n",
    "\n",
    "# results = run_evaluation(data_dict, 100, [trans_rel_1_full], idx_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing searches on the indices we want to use\n",
    "\n",
    "we want to use\n",
    "\n",
    "IVF, PQ, HNSW, LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexConfig(BaseModel):\n",
    "    index_type: str\n",
    "    index_args: List[Any] = []\n",
    "    index_kwargs: Dict[str, Any] = {}\n",
    "\n",
    "trans_rel_lp_cfg = load_config_from_launchplan(r\"sms\\exp1\\runs\\transformer_rel_1\\original_launchplan.yaml\")\n",
    "\n",
    "trans_rel_1_full = ModelEvalConfig(\n",
    "    name=\"trans_rel_1_full\",\n",
    "    lp_config=trans_rel_lp_cfg,\n",
    "    mod_path=r\"sms\\exp1\\runs\\transformer_rel_1\\pretrain_saved_model.pth\",\n",
    "    path_type='full',\n",
    "    use_full_model=True\n",
    ")\n",
    "\n",
    "model = build_model(trans_rel_lp_cfg.model_dump(), full_model_path=r\"sms\\exp1\\runs\\transformer_rel_1\\pretrain_saved_model.pth\", use_full_model=trans_rel_1_full.use_full_model)\n",
    "embeddings_dict = create_embedding_dict(data_dict, trans_rel_lp_cfg.model_dump(), model)\n",
    "idx = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexFlatL2\", index_args=[dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "dim = list(embeddings_dict.values())[0].shape[0]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# flat\n",
    "idx_flat = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexFlatL2\", index_args=[dim])\n",
    "# IVF\n",
    "quantizer = faiss.IndexFlatL2(dim)\n",
    "nlist = np.sqrt(len(embeddings_dict))\n",
    "idx_IVF = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexIVFFlat\", index_args=[quantizer, int(dim), int(nlist)])\n",
    "# PQ (cant do radius search)\n",
    "M = 8\n",
    "nbits = 8\n",
    "idx_PQ = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexPQ\", index_args=[dim, M, nbits]) \n",
    "# HNSW\n",
    "M = 32\n",
    "idx_HNSW = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexHNSWFlat\", index_args=[dim, M])\n",
    "# LSH (cant do radius search)\n",
    "nbits = 64\n",
    "idx_LSH = embeddings_to_faiss_index(embeddings_dict, index_type=\"IndexLSH\", index_args=[dim, nbits])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80692863\n"
     ]
    }
   ],
   "source": [
    "v1 = list(embeddings_dict.values())[0]\n",
    "v2 = list(embeddings_dict.values())[5]\n",
    "i2 = list(embeddings_dict.keys())[5]\n",
    "\n",
    "diff = np.linalg.norm(v1 - v2)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             data_path num_loops model_config_paths\n",
      "                             [model_config_paths ...] output_path\n",
      "ipykernel_launcher.py: error: the following arguments are required: data_path, num_loops, model_config_paths, output_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from sms.src.log import configure_logging\n",
    "from sms.src.vector_search.evaluate_top_k import create_augmented_data, build_model, create_embedding_dict, embeddings_to_faiss_index, evaluate_top_k\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from sms.exp1.config_classes import LaunchPlanConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "configure_logging()\n",
    "\n",
    "class ModelEvalConfig(BaseModel):\n",
    "    name: str\n",
    "    lp_config: LaunchPlanConfig\n",
    "    mod_path: str\n",
    "    path_type: str    #'full' or 'encoder'\n",
    "    use_full_model: bool\n",
    "\n",
    "class IndexConfig(BaseModel):\n",
    "    index_type: str\n",
    "    index_args: List[Any] = []\n",
    "    index_kwargs: Dict[str, Any] = {}\n",
    "\n",
    "def run_evaluation(\n",
    "    data_dict: Dict[str, np.ndarray],\n",
    "    num_loops: int,\n",
    "    model_configs: List[ModelEvalConfig],\n",
    "    index_configs: List[IndexConfig]\n",
    "    ) -> Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]]:\n",
    "\n",
    "    \"\"\"\n",
    "    An extension of the evaluation code in exp1.\n",
    "    Runs topK evaluation for each model config and each index config.\n",
    "\n",
    "    Returns a results dictionary, which has the following structure:\n",
    "    {\n",
    "        model_name: {\n",
    "            index_name: {\n",
    "                topK: {\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1': []\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        data_dict: dictionary of data, where each value is a numpy array.\n",
    "        num_loops: number of loops to run.\n",
    "        model_configs: list of model configurations.\n",
    "        index_configs: list of index configurations.\n",
    "\n",
    "    Returns:\n",
    "        results: dictionary of results, where each value is a dictionary of topK evaluation results.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate random augmentations\n",
    "    anchor_keys = np.random.choice(list(data_dict.keys()), size=num_loops, replace=False)\n",
    "    augmented_data = create_augmented_data(data_dict, anchor_keys)\n",
    "\n",
    "    results = {}\n",
    "    for eval_config in model_configs:\n",
    "        logger.info(f\"Running evaluation for {eval_config.name}\")\n",
    "\n",
    "        dumped_lp_config = eval_config.lp_config.model_dump()\n",
    "        bm_cfg = {'full_model_path': eval_config.mod_path} if eval_config.path_type == 'full' else {'encoder_path': eval_config.mod_path}\n",
    "\n",
    "        model = build_model(dumped_lp_config, **bm_cfg, use_full_model=eval_config.use_full_model)\n",
    "        embeddings_dict = create_embedding_dict(data_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created embedding dictionary for {len(embeddings_dict)} keys.\")\n",
    "\n",
    "        # create augmented embeddings structure\n",
    "        augmented_embeddings_dict = {}\n",
    "        for data_id, aug_dict in augmented_data.items():\n",
    "            augmented_embeddings_dict[data_id] = create_embedding_dict(aug_dict, dumped_lp_config, model)\n",
    "        logger.info(f\"Created augmented embeddings.\")\n",
    "\n",
    "        dim = list(embeddings_dict.values())[0].shape[0]\n",
    "\n",
    "        #TODO: record embedding dimension\n",
    "\n",
    "        # FLATL2 baseline \n",
    "        index_config = IndexConfig(index_type=\"IndexFlatL2\", index_args=[dim])\n",
    "        index = embeddings_to_faiss_index(embeddings_dict=embeddings_dict, **index_config.model_dump())\n",
    "        logger.info(f\"Created FAISS index with parameters {index_config.model_dump()}\")\n",
    "        results[eval_config.name] = evaluate_top_k(embeddings_dict, augmented_embeddings_dict, [1, 3, 5, 10, 25, 50, 100], index)\n",
    "        logger.info(f\"Evaluated top K.\")\n",
    "        #TODO: add to CustomFAISSINdex the details of the index, like bytes used for each embedding, databse memory usage.\n",
    "\n",
    "        #TODO: make sure timings are recorded\n",
    "        for index_config in index_configs:\n",
    "            index_config_dict = index_config.model_dump()\n",
    "            index = embeddings_to_faiss_index(embeddings_dict=embeddings_dict, **index_config_dict)\n",
    "            logger.info(f\"Created FAISS index with parameters {index_config_dict}\")\n",
    "            results[eval_config.name] = evaluate_top_k(embeddings_dict, augmented_embeddings_dict, [1, 3, 5, 10, 25, 50, 100], index)\n",
    "            logger.info(f\"Evaluated top K.\")\n",
    "    return results\n",
    "\n",
    "def main(data_path: str, num_loops: int, model_config_paths: List[str], output_path: str):\n",
    "    data_dict = pkl.load(open(data_path, 'rb'))\n",
    "    model_configs = []\n",
    "    for config_path in model_config_paths:\n",
    "        with open(config_path, 'r') as file:\n",
    "            config_data = yaml.safe_load(file)\n",
    "        try:\n",
    "            model_config = ModelEvalConfig(**config_data)\n",
    "            model_configs.append(model_config)\n",
    "        except pydantic.ValidationError as e:\n",
    "            logger.error(f\"Invalid configuration in {config_path}: {e}\")\n",
    "            raise\n",
    "    results = run_evaluation(data_dict, num_loops, model_configs)\n",
    "    pkl.dump(results, open(output_path, 'wb'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run model evaluation.\")\n",
    "    parser.add_argument('data_path', type=str, help='Path to the data file.')\n",
    "    parser.add_argument('num_loops', type=int, help='Number of loops for evaluation.')\n",
    "    parser.add_argument('model_config_paths', type=str, nargs='+', help='Paths to model configuration files.')\n",
    "    parser.add_argument('output_path', type=str, help='Path to the output file.')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args.data_path, args.num_loops, args.model_config_paths, args.output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
