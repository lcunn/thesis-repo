{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.chdir(\"c:/Users/cunn2/OneDrive/DSML/Project/thesis-repo\")\n",
    "\n",
    "from sms.exp1.config_classes import load_config_from_launchplan\n",
    "from sms.exp1.run_training import build_encoder, build_projector\n",
    "from sms.exp1.models.siamese import SiameseModel\n",
    "\n",
    "# config = load_config_from_launchplan(\"sms/exp1/runs/run_20240926_162652/original_launchplan.yaml\")\n",
    "\n",
    "# encoder = build_encoder(config.model_dump())\n",
    "# projector = build_projector(config.model_dump())\n",
    "\n",
    "# model = SiameseModel(encoder, projector)\n",
    "\n",
    "# print(encoder)\n",
    "# print(projector)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cunn2\\AppData\\Local\\Temp\\ipykernel_288856\\2288947575.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\data\\exp1\\val_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "# bert\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class TokenAndPositionalEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.token_emb = nn.Conv1d(self.input_dim, self.emb_dim, 1)\n",
    "        self.pos_emb = self.positional_encoding(self.max_len, self.emb_dim)\n",
    "\n",
    "    def get_angles(self, pos, i, emb_dim):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(emb_dim))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, emb_dim):\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(emb_dim)[np.newaxis, :],\n",
    "            emb_dim,\n",
    "        )\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x = self.token_emb(x)\n",
    "        x *= torch.sqrt(torch.tensor(self.emb_dim, dtype=torch.float32))\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        return x + self.pos_emb.to(x.device)[:, : x.shape[1]]\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config, input_shape=2, d_latent=64):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.d_input = input_shape\n",
    "        self.d_latent = d_latent\n",
    "        self.d_model = config.get(\"d_model\", 128)\n",
    "        self.n_layers = config.get(\"n_layers\", 4)\n",
    "\n",
    "        self.emb = TokenAndPositionalEmbeddingLayer(\n",
    "            input_dim=self.d_input, emb_dim=self.d_model, max_len=config.get(\"max_seq_len\", 512)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=config.get(\"n_heads\", 8),\n",
    "            dim_feedforward=config.get(\"d_ff\", self.d_model * 4),\n",
    "            dropout=config.get(\"dropout_rate\", 0.1),\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=self.n_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.d_model, self.d_latent)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # (assuming input batch has shape [batch_size, padded_seq_length, point_dim])\n",
    "        # batch_key_padding_mask are all False, so the output is the same as batch. This is because all inputs have the same length.\n",
    "        batch_key_padding_mask = torch.zeros((batch.shape[0], batch.shape[1])).bool()\n",
    "        batch_key_padding_mask = batch_key_padding_mask.to(batch.device)\n",
    "        batch_emb = self.emb(batch)             # (batch_size, padded_seq_length, d_model)\n",
    "        batch_emb = self.transformer_encoder(\n",
    "            batch_emb, batch_key_padding_mask=batch_key_padding_mask\n",
    "        )                                       # (batch_size, padded_seq_length, d_model)\n",
    "        batch_emb = self.fc(batch_emb)          # (batch_size, padded_seq_length, d_latent)\n",
    "        batch_emb = torch.permute(batch_emb, (0, 2, 1))  # (batch_size, d_latent, padded_seq_length)\n",
    "        batch_emb = self.pool(batch_emb)            # (batch_size, d_latent, 1)\n",
    "        batch_emb = torch.squeeze(batch_emb, dim=2)  # (batch_size, d_latent)\n",
    "\n",
    "        return batch_emb\n",
    "    \n",
    "data = torch.load(r\"C:\\Users\\cunn2\\OneDrive\\DSML\\Project\\thesis-repo\\data\\exp1\\val_data.pt\")\n",
    "max_length = max([len(chunk) for chunk in data])\n",
    "\n",
    "dumped_lp_config = {\n",
    "    \"encoder\": {\n",
    "        \"type\": \"BertEncoder\",\n",
    "        \"params\": {\n",
    "            \"config\": {\n",
    "                \"d_model\": 128,\n",
    "                \"n_layers\": 4,\n",
    "                \"n_heads\": 8,\n",
    "                \"d_ff\": 512,\n",
    "                \"d_expander\": 256,\n",
    "                \"dropout_rate\": 0.1,\n",
    "                \"max_seq_len\": 512\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"dims\": {\n",
    "        \"input_shape\": 2,\n",
    "        \"d_latent\": 64\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"make_relative_pitch\": True,\n",
    "        \"normalize_octave\": False,\n",
    "        \"piano_roll\": False,\n",
    "        \"quantize\": False,\n",
    "        \"rest_pitch\": -1,\n",
    "        \"steps_per_bar\": 32,\n",
    "        \"pad_sequence\": True,\n",
    "        \"pad_val\": -1000,\n",
    "        \"goal_seq_len\": max_length\n",
    "    }\n",
    "}\n",
    "\n",
    "encoder = build_encoder(dumped_lp_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (emb): TokenAndPositionalEmbeddingLayer(\n",
       "    (token_emb): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sms.src.synthetic_data.formatter import InputFormatter\n",
    "\n",
    "data_ex = data[:10]\n",
    "\n",
    "formatter = InputFormatter(**dumped_lp_config['input'])\n",
    "formatted_data_list = [torch.from_numpy(formatter(chunk).astype(np.float32).copy()) for chunk in data_ex]\n",
    "formatted_data_stacked = torch.stack(formatted_data_list, dim=0) # shape [num_chunks, *input_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3793e-01,  3.5387e-01,  1.0780e-02,  1.2166e+00,  4.6569e-01,\n",
       "          3.8973e-01, -6.7486e-01,  1.0874e-02,  2.3693e-01,  1.0819e-02,\n",
       "         -9.4808e-01,  4.6273e-01, -2.2738e-01, -6.4120e-01, -2.5948e-01,\n",
       "          3.9248e-01, -1.1037e+00,  4.0670e-01,  2.2948e-01, -2.9387e-01,\n",
       "          7.1413e-01, -9.0243e-01, -4.2870e-02,  6.1272e-01, -3.8800e-01,\n",
       "          5.6685e-01, -5.9928e-01, -2.3420e-01,  2.5666e-01, -6.5252e-01,\n",
       "         -4.7824e-01, -2.6841e-02, -2.1075e-01,  1.1169e+00, -7.0040e-01,\n",
       "         -6.6192e-01,  2.1128e-01, -5.6558e-01, -1.2892e-03, -1.0727e+00,\n",
       "         -1.7289e-01, -1.8228e-01,  8.1735e-01,  1.1020e-01,  4.5572e-01,\n",
       "         -2.2616e-01, -6.2268e-01, -1.3244e-01, -3.4373e-01,  4.2239e-01,\n",
       "          1.5603e-01, -1.3300e-01, -2.6821e-01,  3.8612e-01, -2.0867e-01,\n",
       "         -1.9862e-01,  2.0648e-01,  8.5957e-01, -2.5489e-01,  9.9180e-01,\n",
       "         -3.7443e-01, -1.8460e-04,  2.0290e-01,  3.9178e-01],\n",
       "        [-9.3135e-01, -2.2460e-02, -9.4327e-02,  1.1142e+00,  3.4571e-01,\n",
       "          6.5395e-01, -4.6125e-01,  2.5740e-01, -1.4868e-01, -1.1959e-01,\n",
       "         -8.2362e-01, -2.3967e-02, -1.1353e-01, -3.5504e-01, -5.2613e-01,\n",
       "          3.7896e-01, -5.1318e-01,  4.4202e-01, -3.0245e-01, -2.5767e-01,\n",
       "          3.1128e-01, -9.8320e-01,  1.6961e-01,  6.9408e-01, -3.7517e-01,\n",
       "         -2.4482e-02, -4.2675e-01, -1.5364e-01, -9.8582e-02, -6.9676e-01,\n",
       "         -5.8607e-01, -5.6029e-02, -2.3596e-01,  1.0076e+00,  2.4472e-02,\n",
       "         -1.2161e-01, -4.8918e-02, -3.2071e-01, -7.5314e-02, -1.0663e+00,\n",
       "          2.0774e-01, -5.2046e-01,  6.5135e-01,  1.6691e-01,  5.5314e-01,\n",
       "         -2.7135e-01, -5.4394e-01, -1.0668e-01, -2.1751e-01,  3.5351e-01,\n",
       "         -1.8787e-02, -1.7697e-01, -1.2393e-01,  8.1631e-01, -1.2152e-02,\n",
       "         -2.7946e-01, -4.4713e-01,  1.0675e+00, -2.0639e-01,  8.5264e-01,\n",
       "          1.0864e-01,  6.2214e-01,  2.7162e-01,  2.8681e-01],\n",
       "        [-6.3931e-01,  5.2494e-02, -6.4464e-03,  1.0064e+00,  1.6899e-01,\n",
       "          8.5706e-01, -5.8302e-01,  2.2019e-01, -7.7374e-02,  3.2400e-01,\n",
       "         -1.1598e+00,  4.2656e-01,  4.7213e-02, -4.8518e-01, -5.8048e-01,\n",
       "          1.2351e-01, -1.0624e+00,  1.9386e-01, -1.2038e-01, -1.7556e-01,\n",
       "          7.5970e-01, -5.7549e-01, -1.3391e-03,  9.1834e-01, -5.3049e-01,\n",
       "          2.5214e-01, -6.1422e-01, -4.1866e-01,  3.1369e-01, -9.4376e-01,\n",
       "         -9.4697e-01,  2.1329e-01,  7.0825e-02,  1.0936e+00, -6.1996e-01,\n",
       "         -5.7840e-01,  1.0223e-01, -4.1395e-01, -7.2010e-02, -1.2319e+00,\n",
       "         -3.3893e-01, -2.1779e-01,  4.2597e-01,  9.8509e-02,  4.2612e-01,\n",
       "         -2.4062e-01, -6.8568e-01, -1.3641e-02, -4.3507e-01,  3.3570e-01,\n",
       "          5.5888e-01, -1.6385e-01, -9.6117e-02,  6.7546e-01,  7.8043e-02,\n",
       "         -3.1694e-01,  2.4472e-01,  7.3111e-01,  7.2035e-02,  9.2469e-01,\n",
       "         -5.0234e-01,  4.6501e-01,  2.5288e-01,  3.0558e-02],\n",
       "        [-7.8471e-01,  7.7159e-02,  7.0961e-03,  9.6059e-01, -4.3211e-02,\n",
       "          2.7724e-01, -6.9000e-01,  1.4711e-01, -2.2514e-01,  1.1663e-01,\n",
       "         -7.9463e-01,  2.5992e-01,  5.2708e-01, -4.8020e-01, -7.5558e-01,\n",
       "          5.1589e-01, -5.8352e-01,  1.6810e-01, -1.5871e-01,  1.4878e-01,\n",
       "          1.0941e-01, -4.2711e-01,  9.3938e-02,  5.0860e-01, -1.7416e-01,\n",
       "          1.0361e-01, -4.3117e-01, -1.1696e-01, -4.3793e-02, -7.3416e-01,\n",
       "         -6.8740e-01,  2.9244e-01, -5.4977e-02,  8.2697e-01, -1.4207e-01,\n",
       "          3.7998e-02, -9.9911e-02, -2.9437e-01,  2.8719e-02, -8.8820e-01,\n",
       "         -2.5999e-01, -1.4384e-01,  7.6379e-01,  1.5114e-01,  4.1564e-01,\n",
       "         -1.4594e-01, -3.2354e-01, -8.5621e-02, -3.2188e-01,  3.3681e-02,\n",
       "         -4.0721e-02, -5.7913e-01,  2.9346e-02,  6.6782e-01,  5.7818e-03,\n",
       "         -3.7332e-02, -8.0544e-02,  5.6127e-01,  3.3684e-01,  9.5560e-01,\n",
       "         -6.6726e-01,  2.9533e-01,  1.5669e-01,  4.2790e-01],\n",
       "        [-8.2569e-01,  1.8532e-01,  1.1935e-01,  1.1567e+00,  1.7900e-01,\n",
       "          3.8832e-01, -6.5270e-01,  2.6711e-01,  3.6943e-02,  3.1364e-02,\n",
       "         -8.2208e-01,  1.0464e-01,  9.9975e-03, -4.1643e-01, -6.0603e-01,\n",
       "          6.5473e-01, -8.6689e-01,  1.9592e-01,  3.2494e-03, -9.9425e-02,\n",
       "          4.4525e-01, -7.7453e-01,  4.9359e-02,  7.6532e-01, -2.4662e-01,\n",
       "          3.6388e-01, -3.7422e-01, -1.1882e-01,  1.0196e-01, -7.4913e-01,\n",
       "         -5.3906e-01,  3.2212e-01, -9.7101e-02,  7.4396e-01, -5.2043e-01,\n",
       "         -3.7987e-01, -5.0196e-02, -1.1676e-01,  2.2698e-01, -9.5864e-01,\n",
       "         -1.1835e-01, -5.0102e-01,  7.9265e-01,  1.3276e-01,  5.5794e-01,\n",
       "         -3.1571e-02, -4.2632e-01, -5.6947e-02, -3.1194e-01,  2.9796e-01,\n",
       "          1.3434e-01, -3.3187e-01, -2.0412e-01,  7.2154e-01, -2.6256e-01,\n",
       "         -3.4413e-01, -8.3615e-02,  6.3555e-01,  9.3661e-02,  8.4027e-01,\n",
       "         -3.2798e-01,  1.0863e-01,  1.4334e-01,  2.5080e-01],\n",
       "        [-6.9008e-01,  3.9801e-01, -1.7693e-02,  1.4071e+00,  2.5033e-01,\n",
       "          6.0014e-01, -5.8731e-01,  1.6010e-01,  9.7126e-02, -1.9763e-01,\n",
       "         -1.0725e+00,  4.5373e-01, -1.3405e-01, -7.1750e-01, -2.2024e-01,\n",
       "          3.8482e-01, -9.4206e-01,  4.1395e-01,  6.1779e-02, -3.4659e-01,\n",
       "          5.7750e-01, -9.0334e-01, -1.0770e-01,  7.7162e-01, -4.9175e-01,\n",
       "          4.6327e-01, -6.4991e-01, -3.1940e-01,  8.5133e-02, -6.0712e-01,\n",
       "         -6.4969e-01,  1.8442e-02, -3.3946e-01,  1.0076e+00, -5.3681e-01,\n",
       "         -5.8692e-01,  6.9658e-02, -4.2662e-01, -2.9424e-03, -8.2694e-01,\n",
       "         -6.3763e-02, -4.5871e-01,  6.2509e-01, -1.5281e-01,  3.5881e-01,\n",
       "         -3.3329e-01, -6.1537e-01,  6.6266e-02, -4.7724e-01,  4.0238e-01,\n",
       "          3.1749e-01, -3.8429e-01, -3.4655e-02,  5.2858e-01, -4.7853e-01,\n",
       "          6.2494e-02,  1.2360e-01,  7.5788e-01, -2.6822e-01,  8.7095e-01,\n",
       "         -4.4448e-01,  5.5420e-02,  2.7218e-01,  3.5568e-01],\n",
       "        [-1.0348e+00, -1.5157e-01, -2.7163e-03,  1.1563e+00,  5.4247e-01,\n",
       "          1.4093e-01, -4.7804e-01, -1.0478e-01, -3.9685e-01,  4.9542e-02,\n",
       "         -7.1153e-01, -5.2099e-02,  3.7780e-01, -6.0446e-01, -6.6081e-01,\n",
       "          2.2759e-01, -7.1253e-01,  7.0891e-01, -1.1759e-01, -1.0844e-01,\n",
       "          4.1197e-01, -1.1548e+00, -2.6305e-02,  8.0944e-01, -4.3265e-01,\n",
       "          1.0989e-01, -5.0426e-01, -8.8838e-02, -2.1406e-01, -9.2774e-01,\n",
       "         -8.2045e-01,  1.6688e-01, -1.3976e-01,  7.0553e-01, -1.3843e-01,\n",
       "         -1.6130e-01, -2.3465e-01, -1.5291e-01, -2.1723e-01, -8.4256e-01,\n",
       "         -1.8996e-02, -1.9049e-01,  7.2653e-01,  2.5948e-02,  5.0475e-01,\n",
       "         -2.2940e-01, -4.8440e-01, -3.9839e-01, -3.1322e-01,  1.0854e-01,\n",
       "          1.3169e-01, -5.0015e-01, -7.0983e-02,  1.8389e-01,  7.0394e-02,\n",
       "         -2.7548e-02, -1.8588e-01,  1.1460e+00, -2.0067e-01,  1.2252e+00,\n",
       "         -6.6212e-03,  4.1454e-01,  8.1124e-02,  3.4305e-01],\n",
       "        [-7.9868e-01,  1.5734e-01,  3.1398e-03,  1.1373e+00,  4.0680e-01,\n",
       "          7.3615e-01, -6.2656e-01,  2.4906e-01,  2.1146e-01,  4.9821e-02,\n",
       "         -8.5030e-01,  2.7565e-01, -3.4420e-02, -3.5218e-01, -5.1219e-01,\n",
       "          4.8942e-01, -6.6175e-01,  1.8674e-01, -1.4322e-02, -2.9803e-01,\n",
       "          6.2192e-01, -6.6057e-01, -8.5285e-02,  6.7204e-01, -4.2054e-01,\n",
       "          3.5024e-01, -3.2269e-01, -2.3735e-01,  4.8487e-02, -7.3823e-01,\n",
       "         -5.7270e-01,  3.0205e-01, -2.3017e-02,  1.0889e+00, -4.2510e-01,\n",
       "         -5.0915e-01, -4.0197e-02, -1.7261e-01, -1.4366e-01, -9.5520e-01,\n",
       "         -1.2740e-01, -4.9606e-01,  5.3897e-01,  1.5678e-01,  4.8129e-01,\n",
       "         -1.8829e-01, -3.4896e-01,  3.7916e-02, -9.3042e-02,  1.4428e-01,\n",
       "          1.2754e-01, -3.3540e-01,  6.2230e-03,  7.2940e-01, -9.5442e-02,\n",
       "         -2.6754e-01,  1.6499e-03,  7.6810e-01, -4.9772e-02,  9.4572e-01,\n",
       "         -2.7623e-01,  1.8643e-01,  4.4465e-01,  3.5592e-01],\n",
       "        [-9.4771e-01,  3.3281e-02, -1.1485e-01,  1.2265e+00,  9.7734e-02,\n",
       "          9.5658e-01, -6.6702e-01,  3.1498e-01, -1.2437e-01, -1.2825e-01,\n",
       "         -1.0280e+00,  3.8126e-02, -2.4311e-02, -5.4858e-01, -5.8010e-01,\n",
       "          5.1245e-01, -7.1130e-01,  2.8787e-01, -2.0839e-01, -3.5376e-01,\n",
       "          3.5269e-01, -5.8601e-01,  5.2691e-02,  7.8151e-01, -3.9545e-01,\n",
       "          2.4583e-01, -4.9552e-01, -2.2224e-01,  8.4219e-04, -6.8110e-01,\n",
       "         -5.5901e-01,  3.5792e-02, -4.8596e-02,  9.7166e-01, -2.4017e-01,\n",
       "         -3.4066e-01,  5.2706e-02, -1.6359e-01, -1.7571e-01, -8.8017e-01,\n",
       "          4.2409e-02, -6.9337e-01,  4.4071e-01,  3.7313e-01,  3.9812e-01,\n",
       "         -2.5977e-01, -6.6613e-01,  2.6632e-01, -2.9378e-01,  3.3782e-01,\n",
       "          3.1002e-01, -2.9565e-01, -4.8331e-02,  8.8334e-01, -1.4973e-01,\n",
       "         -1.9735e-01, -1.1896e-01,  5.8689e-01,  2.1703e-03,  7.7582e-01,\n",
       "         -2.6222e-01,  2.9198e-01,  4.1310e-01,  1.3104e-01],\n",
       "        [-4.3995e-01,  1.9925e-01,  1.3028e-01,  7.7365e-01,  9.4672e-02,\n",
       "          5.7752e-01, -6.9666e-01,  1.5236e-01,  8.6410e-02,  1.7915e-01,\n",
       "         -1.1134e+00,  4.2257e-01,  1.2066e-01, -5.4465e-01, -4.4956e-01,\n",
       "          2.2956e-01, -1.0068e+00,  4.7987e-02,  2.4491e-01, -1.6736e-01,\n",
       "          7.9545e-01, -4.2762e-01, -1.8676e-01,  7.7972e-01, -4.7291e-01,\n",
       "          5.8083e-01, -3.5222e-01, -2.6952e-01,  2.3300e-01, -8.9798e-01,\n",
       "         -7.2813e-01,  2.3913e-01,  7.1176e-02,  7.8423e-01, -7.2202e-01,\n",
       "         -8.3144e-01, -4.2971e-02, -1.2481e-01,  3.3188e-02, -8.0897e-01,\n",
       "         -5.1164e-01, -2.3058e-01,  4.5396e-01,  1.7748e-01,  2.9403e-01,\n",
       "         -1.3997e-01, -6.5298e-01,  2.9634e-01, -2.5945e-01,  1.7031e-01,\n",
       "          3.9210e-01, -4.3749e-01,  1.4314e-02,  4.7523e-01,  1.3985e-02,\n",
       "         -2.4533e-01,  5.5542e-01,  3.2429e-01, -1.1237e-01,  8.3073e-01,\n",
       "         -7.8404e-01,  8.8997e-02,  3.7275e-01,  1.9440e-01]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(formatted_data_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2 71. ]\n",
      " [ 3.  69. ]\n",
      " [ 0.8 74. ]]\n",
      "[[ 2.0e-01  7.1e+01]\n",
      " [ 3.0e+00  6.9e+01]\n",
      " [ 8.0e-01  7.4e+01]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]\n",
      " [-1.0e+03 -1.0e+03]]\n"
     ]
    }
   ],
   "source": [
    "formatter = InputFormatter(pad_sequence=True)\n",
    "\n",
    "print(data[0])\n",
    "print(formatter(data[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
